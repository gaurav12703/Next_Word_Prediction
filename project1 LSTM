import numpy as np
import pandas as pd

file=open("/kaggle/input/next-word-prediction/1661-0.txt")
text=file.read()

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
tokenizer=Tokenizer()
tokenizer.fit_on_texts([text])
len(tokenizer.word_index)

input_sequence=[]
for sentence in text.split('\n'):
    print(sentence)
    print(tokenizer.texts_to_sequences([sentence])[0])
    tokenized_sentence=tokenizer.texts_to_sequences([sentence])[0]
    
    for i in range(1,len(tokenized_sentence)):
        input_sequence.append(tokenized_sentence[:i+1])

input_sequence

max_len=max([len(x) for x in input_sequence])
print(max_len)

from tensorflow.keras.preprocessing.sequence import pad_sequences
padded_input_sequences=pad_sequences(input_sequence,maxlen=max_len,padding='pre')

padded_input_sequences
x=padded_input_sequences[:,:-1]
print(x)
y=padded_input_sequences[:,-1]
print(y)

from tensorflow.keras.utils import to_categorical
y=to_categorical(y,num_classes=8932)
print(y.shape)
y

#BUILDING LSTM MODEL 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Embedding , LSTM,Dense

model=Sequential()
model.add(Embedding(8932,100,input_length=19))
model.add(LSTM(150))
model.add(Dense(8932, activation='softmax'))

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

model.fit(x,y,epochs=100)

import time 
text="This eBook is for the  "
for i in range(3):
    #tokenize
    token_text=tokenizer.texts_to_sequences([text])[0]
    #padding
    padded_token_text=pad_sequences([token_text],maxlen=19,padding='pre')
    #predict
    pos=np.argmax(model.predict(padded_token_text))
    
    for word,index  in tokenizer.word_index.items():
        if index==pos:
            text=text+" "+word
            print(text)
            time.sleep(1)
